#ä»»åŠ¡åˆ†é…ï¼š
#å­Ÿç¥¥æ”¿ï¼šnlpè™šæ‹Ÿç¯å¢ƒé…ç½®ï¼Œæ¥å…¥è±†åŒ…APIé…ç½®ï¼ŒæœåŠ¡å™¨è¿è¡ŒAIQAé…ç½®ï¼Œapp.pyç¼–å†™ï¼Œç•Œé¢æ¨¡å—HTMLç¼–å†™ï¼Œæœºå™¨ç¿»è¯‘æ¨¡å‹è®­ç»ƒ
#å”ä¿Šæ°ï¼šæƒ…æ„Ÿåˆ†ææ¨¡å‹ä¼˜åŒ–ä¸è®­ç»ƒï¼ˆå‡†ç¡®ç‡0.9ä»¥ä¸Šï¼‰ï¼Œç•Œé¢æ¨¡å—HTMLç¼–å†™
#9ç­ç‹å¤©å®‡ï¼šæ–‡æœ¬åˆ†ç±»æ¨¡å‹ä¼˜åŒ–ä¸è®­ç»ƒï¼ˆå‡†ç¡®ç‡0.85ä»¥ä¸Šï¼‰ï¼Œæƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒ
#é©¬åº·è¶…ï¼šæœºå™¨ç¿»è¯‘æ¨¡å‹ä¼˜åŒ–ä¸è®­ç»ƒï¼ˆå‡†ç¡®ç‡0.85ä»¥ä¸Šï¼‰ï¼Œæ–‡æœ¬åˆ†ç±»æ¨¡å‹è®­ç»ƒ
# -*- coding: utf-8 -*-
import os
import sys
import http.client
import json
import time
import threading
from typing import List, Tuple, Optional, Dict

import numpy as np
import pandas as pd
import jieba
import tensorflow as tf
from flask import Flask, render_template, request, jsonify
from tensorflow import keras
from tensorflow.keras.preprocessing import sequence
# ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨tf.kerasè€Œä¸æ˜¯å•ç‹¬çš„keras
from tensorflow.keras.models import load_model

# å¯¼å…¥è®­ç»ƒæ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), 'nlp_deeplearn', 'code'))
try:
    from importlib import import_module
    # åŠ¨æ€å¯¼å…¥è®­ç»ƒæ¨¡å—
    _training_modules = {}
except:
    pass

# --------------------------- è·¯å¾„ä¸å…¨å±€èµ„æº --------------------------- #
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
NLP_DIR = os.path.join(BASE_DIR, 'nlp_deeplearn')
DATA_DIR = os.path.join(NLP_DIR, 'data')
TMP_DIR = os.path.join(NLP_DIR, 'tmp')

# æ–‡æœ¬åˆ†ç±»
_cls_model: Optional[tf.keras.Model] = None
_cls_word_to_id: Optional[Dict[str, int]] = None
_cls_categories: List[str] = ['ä½“è‚²', 'è´¢ç»', 'æˆ¿äº§', 'å®¶å±…', 'æ•™è‚²', 'ç§‘æŠ€', 'æ—¶å°š', 'æ—¶æ”¿', 'æ¸¸æˆ', 'å¨±ä¹']
_CLS_SEQ_LEN = 600

# æƒ…æ„Ÿåˆ†æ
_sent_model: Optional[tf.keras.Model] = None
_sent_dicts: Optional[pd.DataFrame] = None
_SENT_MAXLEN = 50

# æœºå™¨ç¿»è¯‘
_enc_model = None
_dec_model = None
_inp_lang_tokenizer = None
_targ_lang_tokenizer = None
_max_length_targ = None
_max_length_inp = None

# è®­ç»ƒçŠ¶æ€
_training_status = {
    'text_classification': {'status': 'idle', 'progress': 0, 'message': ''},
    'sentiment': {'status': 'idle', 'progress': 0, 'message': ''},
    'translation': {'status': 'idle', 'progress': 0, 'message': ''}
}


def _safe_path(*paths: str) -> str:
    """æ„å»ºç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„é—®é¢˜ã€‚"""
    return os.path.join(BASE_DIR, *paths)


# ---------- æ–‡æœ¬åˆ†ç±»ï¼šåŠ è½½ä¸é¢„æµ‹ ---------- #
def _load_text_classification_assets():
    global _cls_model, _cls_word_to_id
    if _cls_model is not None and _cls_word_to_id is not None:
        return

    vocab_path = _safe_path('nlp_deeplearn', 'data', 'cnews.vocab.txt')
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ¨¡å‹è·¯å¾„
    model_paths = [
        _safe_path('nlp_deeplearn', 'tmp', 'text_classification_best.h5'),
        _safe_path('nlp_deeplearn', 'tmp', 'text_classification_final.h5'),
        _safe_path('nlp_deeplearn', 'tmp', 'my_model.h5')
    ]
    
    model_path = None
    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if not os.path.exists(vocab_path) or model_path is None:
        raise FileNotFoundError('æ–‡æœ¬åˆ†ç±»æ¨¡å‹æˆ–è¯è¡¨æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè®­ç»ƒå¹¶ä¿å­˜ã€‚')

    with open(vocab_path, encoding='utf-8') as f:
        words = [line.strip() for line in f.readlines()]
    _cls_word_to_id = dict(zip(words, range(len(words))))
    _cls_model = load_model(model_path)


def classify_text(text: str) -> str:
    """å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œ10ç±»æ–°é—»åˆ†ç±»ã€‚"""
    _load_text_classification_assets()
    ids = [_cls_word_to_id[ch] for ch in text if ch in _cls_word_to_id]
    if not ids:
        return 'å†…å®¹è¿‡çŸ­æˆ–æ— æœ‰æ•ˆè¯æ±‡'
    x_pad = sequence.pad_sequences([ids], _CLS_SEQ_LEN)
    prob = _cls_model.predict(x_pad, verbose=0)
    idx = int(np.argmax(prob, axis=1)[0])
    return _cls_categories[idx]


# ---------- æƒ…æ„Ÿåˆ†æï¼šåŠ è½½ä¸é¢„æµ‹ ---------- #
def _build_sentiment_dicts() -> pd.DataFrame:
    """å¤ç°10_3_2.pyä¸­çš„è¯å…¸æ„å»ºæµç¨‹ï¼Œä¿è¯ä¸è®­ç»ƒä¸€è‡´ã€‚"""
    # ä½¿ç”¨_safe_pathç¡®ä¿è·¯å¾„æ­£ç¡®
    pos_path = _safe_path('nlp_deeplearn', 'data', 'pos.xls')
    neg_path = _safe_path('nlp_deeplearn', 'data', 'neg.xls')
    sum_path = _safe_path('nlp_deeplearn', 'data', 'sum.xls')
    
    pos = pd.read_excel(pos_path, header=None, index_col=None)
    neg = pd.read_excel(neg_path, header=None, index_col=None)
    pos['mark'] = 1
    neg['mark'] = 0
    pn_all = pd.concat([pos, neg], ignore_index=True)
    pn_all[0] = pn_all[0].astype(str)
    cut_word = lambda x: list(jieba.cut(x))
    pn_all['words'] = pn_all[0].apply(cut_word)
    comment = pd.read_excel(sum_path)
    comment = comment[comment['rateContent'].notnull()]
    comment['words'] = comment['rateContent'].apply(cut_word)
    pn_comment = pd.concat([pn_all['words'], comment['words']], ignore_index=True)
    w: List[str] = []
    for i in pn_comment:
        w.extend(i)
    dicts = pd.DataFrame(pd.Series(w).value_counts())
    dicts['id'] = list(range(1, len(dicts) + 1))
    return dicts


def _load_sentiment_assets():
    global _sent_model, _sent_dicts
    if _sent_model is not None and _sent_dicts is not None:
        return
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ¨¡å‹è·¯å¾„
    model_paths = [
        _safe_path('nlp_deeplearn', 'tmp', 'sentiment_best.h5'),
        _safe_path('nlp_deeplearn', 'tmp', 'sentiment_final.h5'),
        _safe_path('nlp_deeplearn', 'tmp', 'sentiment_model.h5')
    ]
    
    model_path = None
    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        raise FileNotFoundError('æƒ…æ„Ÿåˆ†ææ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè®­ç»ƒå¹¶ä¿å­˜ã€‚')
    _sent_dicts = _build_sentiment_dicts()
    _sent_model = load_model(model_path)


def _sent_to_ids(text: str) -> List[int]:
    cut_word = lambda x: list(jieba.cut(x))
    words = cut_word(str(text))
    ids = []
    for word in words:
        if word in _sent_dicts['id']:
            ids.append(_sent_dicts['id'][word])
    # å¤„ç†ç©ºåˆ—è¡¨çš„æƒ…å†µ
    if not ids:
        ids = [0]
    padded = sequence.pad_sequences([ids], maxlen=_SENT_MAXLEN)[0]
    return list(padded)


def sentiment_predict(text: str) -> Tuple[str, float]:
    """è¿”å›æƒ…æ„Ÿæ ‡ç­¾ä¸ç½®ä¿¡åº¦ã€‚"""
    _load_sentiment_assets()
    ids = _sent_to_ids(text)
    pred = _sent_model.predict(np.array([ids]), verbose=0)[0][0]
    label = 'æ­£å‘' if pred >= 0.5 else 'è´Ÿå‘'
    return label, float(pred)


# ---------- æœºå™¨ç¿»è¯‘ï¼šåŠ è½½ä¸é¢„æµ‹ ---------- #
def _preprocess_sentence(w: str) -> str:
    import re
    w = re.sub(r'([?.!,])', r' \1 ', w)
    w = re.sub(r"[' ']+", ' ', w)
    return '<start> ' + w + ' <end>'


def _tokenize(lang: List[str]):
    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
    lang_tokenizer.fit_on_texts(lang)
    tensor = lang_tokenizer.texts_to_sequences(lang)
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')
    return tensor, lang_tokenizer


def _create_dataset(path: str, num_examples=None):
    import io
    lines = io.open(path, encoding='UTF-8').read().strip().split('\n')
    word_pairs = [[_preprocess_sentence(w) for w in l.split('\t')] for l in lines[:num_examples]]
    return list(zip(*word_pairs))


def _load_translation_assets():
    """åŠ è½½è¯è¡¨ä¸æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»…ç”¨äºæ¨ç†ã€‚"""
    global _enc_model, _dec_model, _inp_lang_tokenizer, _targ_lang_tokenizer
    global _max_length_targ, _max_length_inp
    if all([_enc_model, _dec_model, _inp_lang_tokenizer, _targ_lang_tokenizer, _max_length_targ, _max_length_inp]):
        return

    path_to_file = _safe_path('nlp_deeplearn', 'data', 'en-ch.txt')
    checkpoint_dir = _safe_path('nlp_deeplearn', 'tmp', 'training_checkpoints')
    if not os.path.exists(path_to_file) or not os.path.exists(checkpoint_dir):
        raise FileNotFoundError('æœºå™¨ç¿»è¯‘æ•°æ®æˆ–æ£€æŸ¥ç‚¹æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè®­ç»ƒå¹¶ä¿å­˜ã€‚')

    targ_lang, inp_lang = _create_dataset(path_to_file, 2000)
    input_tensor, _inp_lang_tokenizer = _tokenize(inp_lang)
    target_tensor, _targ_lang_tokenizer = _tokenize(targ_lang)
    _max_length_targ = max(len(t) for t in target_tensor)
    _max_length_inp = max(len(t) for t in input_tensor)

    embedding_dim = 256
    units = 1024
    vocab_inp_size = len(_inp_lang_tokenizer.word_index) + 1
    vocab_tar_size = len(_targ_lang_tokenizer.word_index) + 1

    class Encoder(tf.keras.Model):
        def __init__(self, vocab_size, embedding_dim, enc_units):
            super().__init__()
            self.enc_units = enc_units
            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
            self.gru = tf.keras.layers.GRU(self.enc_units,
                                           return_sequences=True,
                                           return_state=True,
                                           recurrent_initializer='glorot_uniform')

        def call(self, x, hidden):
            x = self.embedding(x)
            output, state = self.gru(x, initial_state=hidden)
            return output, state

        def initialize_hidden_state(self, batch_size):
            return tf.zeros((batch_size, self.enc_units))

    class BahdanauAttention(tf.keras.layers.Layer):
        def __init__(self, units):
            super().__init__()
            self.W1 = tf.keras.layers.Dense(units)
            self.W2 = tf.keras.layers.Dense(units)
            self.V = tf.keras.layers.Dense(1)

        def call(self, query, values):
            hidden_with_time_axis = tf.expand_dims(query, 1)
            score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))
            attention_weights = tf.nn.softmax(score, axis=1)
            context_vector = attention_weights * values
            context_vector = tf.reduce_sum(context_vector, axis=1)
            return context_vector, attention_weights

    class Decoder(tf.keras.Model):
        def __init__(self, vocab_size, embedding_dim, dec_units):
            super().__init__()
            self.dec_units = dec_units
            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
            self.gru = tf.keras.layers.GRU(self.dec_units,
                                           return_sequences=True,
                                           return_state=True,
                                           recurrent_initializer='glorot_uniform')
            self.fc = tf.keras.layers.Dense(vocab_size)
            self.attention = BahdanauAttention(self.dec_units)

        def call(self, x, hidden, enc_output):
            context_vector, attention_weights = self.attention(hidden, enc_output)
            x = self.embedding(x)
            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
            output, state = self.gru(x)
            output = tf.reshape(output, (-1, output.shape[2]))
            x = self.fc(output)
            return x, state, attention_weights

    _enc_model = Encoder(vocab_inp_size, embedding_dim, units)
    _dec_model = Decoder(vocab_tar_size, embedding_dim, units)

    ckpt = tf.train.Checkpoint(encoder=_enc_model, decoder=_dec_model)
    latest = tf.train.latest_checkpoint(checkpoint_dir)
    if not latest:
        raise FileNotFoundError('æœªæ‰¾åˆ°ç¿»è¯‘æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚')
    ckpt.restore(latest).expect_partial()


def translate_sentence(sentence: str) -> str:
    """ä½¿ç”¨å·²è®­ç»ƒçš„Seq2Seqæ¨¡å‹è¿›è¡Œä¸­->è‹±ç¿»è¯‘ã€‚"""
    _load_translation_assets()
    sentence = _preprocess_sentence(sentence)
    inputs = [_inp_lang_tokenizer.word_index.get(i, 0) for i in sentence.split(' ')]
    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=_max_length_inp, padding='post')
    inputs = tf.convert_to_tensor(inputs)
    hidden = _enc_model.initialize_hidden_state(batch_size=1)
    enc_out, enc_hidden = _enc_model(inputs, hidden)
    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([_targ_lang_tokenizer.word_index['<start>']], 0)

    result_words = []
    for _ in range(_max_length_targ):
        predictions, dec_hidden, _ = _dec_model(dec_input, dec_hidden, enc_out)
        predicted_id = int(tf.argmax(predictions[0]).numpy())
        predicted_word = _targ_lang_tokenizer.index_word.get(predicted_id, '')
        if predicted_word == '<end>':
            break
        if predicted_word and predicted_word != '<start>':
            result_words.append(predicted_word)
        dec_input = tf.expand_dims([predicted_id], 0)
    return ' '.join(result_words) if result_words else 'æ— æ³•ç¿»è¯‘'


# ç¡®ä¿templatesæ–‡ä»¶å¤¹å­˜åœ¨
if not os.path.exists('templates'):
    os.makedirs('templates')
    print("å·²åˆ›å»º templates æ–‡ä»¶å¤¹")

# è±†åŒ…APIè°ƒç”¨å‡½æ•°
def call_doubao_api(user_message, system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"):
    """
    è°ƒç”¨è±†åŒ…APIè¿›è¡Œå¯¹è¯
    
    Args:
        user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œé»˜è®¤æ˜¯åŠ©æ‰‹è§’è‰²
        
    Returns:
        APIè¿”å›çš„å“åº”æ–‡æœ¬
    """
    conn = http.client.HTTPSConnection("ark.cn-beijing.volces.com")
    
    payload = json.dumps({
        "model": "doubao-seed-1-6-vision-250815",
        "messages": [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": user_message
            }
        ],
        "stream": False
    })
    
    headers = {
        'Authorization': 'Bearer dbf0a80b-e5b4-470d-b667-f63084e21443',
        'Content-Type': 'application/json',
        'User-Agent': 'Flask-Doubao-Chat/1.0'
    }
    
    try:
        print(f"å‘é€è¯·æ±‚åˆ°è±†åŒ…API: {user_message[:50]}...")
        start_time = time.time()
        
        conn.request("POST", "/api/v3/chat/completions", payload, headers)
        res = conn.getresponse()
        data = res.read()
        
        end_time = time.time()
        print(f"APIå“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"APIå“åº”çŠ¶æ€: {res.status}")
        
        response_json = json.loads(data.decode("utf-8"))
        
        # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„å“åº”ç»“æ„
        if res.status != 200:
            print(f"APIé”™è¯¯å“åº”: {response_json}")
            return f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status}"
        
        # æå–å›å¤å†…å®¹
        if "choices" in response_json and len(response_json["choices"]) > 0:
            reply = response_json["choices"][0]["message"]["content"]
            print(f"è·å–åˆ°å›å¤: {reply[:50]}...")
            return reply
        elif "error" in response_json:
            error_msg = response_json["error"].get("message", "æœªçŸ¥é”™è¯¯")
            print(f"APIè¿”å›é”™è¯¯: {error_msg}")
            return f"æŠ±æ­‰ï¼ŒAPIè¿”å›é”™è¯¯: {error_msg}"
        else:
            print(f"æ„å¤–çš„å“åº”æ ¼å¼: {response_json}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
            
    except json.JSONDecodeError as e:
        print(f"JSONè§£æé”™è¯¯: {e}")
        return "æŠ±æ­‰ï¼ŒæœåŠ¡å“åº”æ ¼å¼é”™è¯¯ã€‚"
    except Exception as e:
        print(f"APIè°ƒç”¨å‡ºé”™: {e}")
        return "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
    finally:
        conn.close()

# å®ä¾‹åŒ–Flaskåº”ç”¨
app = Flask(__name__, static_url_path='/static')


def preload_nlp_models():
    """é¢„åŠ è½½æ‰€æœ‰NLPæ¨¡å‹"""
    try:
        print("å¼€å§‹é¢„åŠ è½½NLPæ¨¡å‹...")
        
        # æ–‡æœ¬åˆ†ç±»æ¨¡å‹
        print(" åŠ è½½æ–‡æœ¬åˆ†ç±»æ¨¡å‹...")
        _load_text_classification_assets()
        print("  æ–‡æœ¬åˆ†ç±»æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æƒ…æ„Ÿåˆ†ææ¨¡å‹
        print(" åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        _load_sentiment_assets()
        print("  æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æœºå™¨ç¿»è¯‘æ¨¡å‹
        print(" åŠ è½½æœºå™¨ç¿»è¯‘æ¨¡å‹...")
        _load_translation_assets()
        print("  æœºå™¨ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆ")
        
        print("æ‰€æœ‰NLPæ¨¡å‹é¢„åŠ è½½å®Œæˆ!")
    except Exception as e:
        print(f"æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {e}")
        print("æ³¨æ„: NLPåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")


@app.route('/message', methods=['POST'])
def reply():
    """
    å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›è±†åŒ…APIçš„å›å¤
    """
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        req_msg = request.form.get('msg', '').strip()
        
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©º
        if not req_msg:
            return jsonify({'text': 'è¯·è¾“å…¥ä¸€äº›å†…å®¹å§~'})
        
        print(f"æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯: {req_msg}")
        
        # è°ƒç”¨è±†åŒ…APIè·å–å›å¤
        res_msg = call_doubao_api(req_msg)
        
        # æ¸…ç†å’Œæ ¼å¼åŒ–å›å¤
        res_msg = res_msg.strip()
        if not res_msg:
            res_msg = 'æˆ‘ä»¬æ¥èŠèŠå¤©å§'
        
        return jsonify({'text': res_msg})
        
    except Exception as e:
        print(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
        return jsonify({'text': 'æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯ã€‚'})


@app.route("/")
def index():
    """
    è¿”å›èŠå¤©ç•Œé¢
    """
    print("è®¿é—®é¦–é¡µ")
    return render_template('index.html')


@app.route('/test', methods=['GET'])
def test_api():
    """
    æµ‹è¯•APIæ˜¯å¦æ­£å¸¸å·¥ä½œ
    """
    try:
        test_message = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        print(f"æµ‹è¯•APIï¼Œå‘é€æ¶ˆæ¯: {test_message}")
        
        response = call_doubao_api(test_message)
        
        result = {
            'status': 'success',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_message': test_message,
            'response': response,
            'service': 'Doubao Chat API'
        }
        
        return jsonify(result)
        
    except Exception as e:
        print(f"æµ‹è¯•APIæ—¶å‡ºé”™: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        })


@app.route('/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    try:
        # ç®€å•æµ‹è¯•APIè¿æ¥
        test_response = call_doubao_api("å¥åº·æ£€æŸ¥", "ä½ åªéœ€è¦å›å¤'å¥åº·æ£€æŸ¥é€šè¿‡'å³å¯")
        
        return jsonify({
            'status': 'healthy',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'service': 'Doubao Chat API',
            'version': '1.0',
            'api_status': 'connected',
            'test_response': test_response[:100] if test_response else 'æ— å“åº”'
        })
        
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })


# --------------------------- NLP æ¨ç†æ¥å£ --------------------------- #
@app.route('/nlp/classify', methods=['POST'])
def api_classify():
    """æ–°é—»æ–‡æœ¬åˆ†ç±»æ¥å£ã€‚"""
    text = (request.json or {}).get('text', '').strip()
    if not text:
        return jsonify({'status': 'error', 'message': 'text ä¸èƒ½ä¸ºç©º'}), 400
    try:
        category = classify_text(text)
        return jsonify({'status': 'success', 'category': category})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/nlp/sentiment', methods=['POST'])
def api_sentiment():
    """æƒ…æ„Ÿåˆ†ææ¥å£ã€‚"""
    text = (request.json or {}).get('text', '').strip()
    if not text:
        return jsonify({'status': 'error', 'message': 'text ä¸èƒ½ä¸ºç©º'}), 400
    try:
        label, score = sentiment_predict(text)
        return jsonify({'status': 'success', 'label': label, 'score': score})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/nlp/translate', methods=['POST'])
def api_translate():
    """Seq2Seq æœºå™¨ç¿»è¯‘æ¥å£ï¼ˆä¸­->è‹±ï¼‰ã€‚"""
    text = (request.json or {}).get('text', '').strip()
    if not text:
        return jsonify({'status': 'error', 'message': 'text ä¸èƒ½ä¸ºç©º'}), 400
    try:
        result = translate_sentence(text)
        return jsonify({'status': 'success', 'translation': result})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


# --------------------------- NLP è®­ç»ƒæ¥å£ --------------------------- #
def _train_text_classification_async():
    """å¼‚æ­¥è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹"""
    try:
        _training_status['text_classification']['status'] = 'training'
        _training_status['text_classification']['message'] = 'å¼€å§‹è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹...'
        
        # å¯¼å…¥è®­ç»ƒæ¨¡å—
        code_dir = _safe_path('nlp_deeplearn', 'code')
        sys.path.insert(0, code_dir)
        from importlib import import_module
        train_module = import_module('10_3_1')
        
        # è®¾ç½®è·¯å¾„
        base_dir = _safe_path('nlp_deeplearn', 'data')
        train_dir = os.path.join(base_dir, 'cnews.train.txt')
        val_dir = os.path.join(base_dir, 'cnews.val.txt')
        test_dir = os.path.join(base_dir, 'cnews.test.txt')
        vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')
        save_dir = _safe_path('nlp_deeplearn', 'tmp')
        
        _training_status['text_classification']['message'] = 'æ­£åœ¨åŠ è½½æ•°æ®...'
        
        # è°ƒç”¨è®­ç»ƒå‡½æ•°
        history, test_results = train_module.train_text_classification_model(
            train_dir=train_dir,
            val_dir=val_dir,
            test_dir=test_dir,
            vocab_dir=vocab_dir,
            save_dir=save_dir,
            vocab_size=5000,
            seq_length=600,
            batch_size=64,
            epochs=20
        )
        
        _training_status['text_classification']['status'] = 'completed'
        _training_status['text_classification']['progress'] = 100
        _training_status['text_classification']['message'] = f'è®­ç»ƒå®Œæˆï¼æµ‹è¯•é›†å‡†ç¡®ç‡: {test_results["accuracy"]:.4f}'
        
        # é‡æ–°åŠ è½½æ¨¡å‹
        global _cls_model, _cls_word_to_id
        _cls_model = None
        _cls_word_to_id = None
        
    except Exception as e:
        _training_status['text_classification']['status'] = 'error'
        _training_status['text_classification']['message'] = f'è®­ç»ƒå¤±è´¥: {str(e)}'
        import traceback
        traceback.print_exc()


def _train_sentiment_async():
    """å¼‚æ­¥è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    try:
        _training_status['sentiment']['status'] = 'training'
        _training_status['sentiment']['message'] = 'å¼€å§‹è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹...'
        
        # å¯¼å…¥è®­ç»ƒæ¨¡å—
        code_dir = _safe_path('nlp_deeplearn', 'code')
        sys.path.insert(0, code_dir)
        from importlib import import_module
        train_module = import_module('10_3_2')
        
        # è®¾ç½®è·¯å¾„
        base_dir = _safe_path('nlp_deeplearn', 'data')
        pos_path = os.path.join(base_dir, 'pos.xls')
        neg_path = os.path.join(base_dir, 'neg.xls')
        comment_path = os.path.join(base_dir, 'sum.xls')
        save_dir = _safe_path('nlp_deeplearn', 'tmp')
        
        _training_status['sentiment']['message'] = 'æ­£åœ¨åŠ è½½æ•°æ®...'
        
        # è°ƒç”¨è®­ç»ƒå‡½æ•°
        history, test_results, dicts = train_module.train_sentiment_model(
            pos_path=pos_path,
            neg_path=neg_path,
            comment_path=comment_path,
            save_dir=save_dir,
            maxlen=50,
            batch_size=16,
            epochs=10
        )
        
        _training_status['sentiment']['status'] = 'completed'
        _training_status['sentiment']['progress'] = 100
        _training_status['sentiment']['message'] = f'è®­ç»ƒå®Œæˆï¼æµ‹è¯•é›†å‡†ç¡®ç‡: {test_results["accuracy"]:.4f}'
        
        # é‡æ–°åŠ è½½æ¨¡å‹
        global _sent_model, _sent_dicts
        _sent_model = None
        _sent_dicts = None
        
    except Exception as e:
        _training_status['sentiment']['status'] = 'error'
        _training_status['sentiment']['message'] = f'è®­ç»ƒå¤±è´¥: {str(e)}'
        import traceback
        traceback.print_exc()


def _train_translation_async():
    """å¼‚æ­¥è®­ç»ƒæœºå™¨ç¿»è¯‘æ¨¡å‹"""
    try:
        _training_status['translation']['status'] = 'training'
        _training_status['translation']['message'] = 'å¼€å§‹è®­ç»ƒæœºå™¨ç¿»è¯‘æ¨¡å‹...'
        
        # å¯¼å…¥è®­ç»ƒæ¨¡å—
        code_dir = _safe_path('nlp_deeplearn', 'code')
        sys.path.insert(0, code_dir)
        from importlib import import_module
        train_module = import_module('10_4')
        
        # è®¾ç½®è·¯å¾„
        data_path = _safe_path('nlp_deeplearn', 'data', 'en-ch.txt')
        save_dir = _safe_path('nlp_deeplearn', 'tmp')
        
        _training_status['translation']['message'] = 'æ­£åœ¨åŠ è½½æ•°æ®...'
        
        # è°ƒç”¨è®­ç»ƒå‡½æ•°
        results = train_module.train_translation_model(
            data_path=data_path,
            save_dir=save_dir,
            num_examples=2000,
            batch_size=64,
            embedding_dim=256,
            units=1024,
            epochs=50
        )
        
        _training_status['translation']['status'] = 'completed'
        _training_status['translation']['progress'] = 100
        _training_status['translation']['message'] = f'è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {results["best_val_loss"]:.4f}'
        
        # é‡æ–°åŠ è½½æ¨¡å‹
        global _enc_model, _dec_model, _inp_lang_tokenizer, _targ_lang_tokenizer
        global _max_length_targ, _max_length_inp
        _enc_model = None
        _dec_model = None
        _inp_lang_tokenizer = None
        _targ_lang_tokenizer = None
        _max_length_targ = None
        _max_length_inp = None
        
    except Exception as e:
        _training_status['translation']['status'] = 'error'
        _training_status['translation']['message'] = f'è®­ç»ƒå¤±è´¥: {str(e)}'
        import traceback
        traceback.print_exc()


@app.route('/nlp/train/text_classification', methods=['POST'])
def api_train_text_classification():
    """è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹æ¥å£"""
    if _training_status['text_classification']['status'] == 'training':
        return jsonify({
            'status': 'error',
            'message': 'æ¨¡å‹æ­£åœ¨è®­ç»ƒä¸­ï¼Œè¯·ç¨å€™...'
        }), 400
    
    try:
        # é‡ç½®çŠ¶æ€
        _training_status['text_classification'] = {'status': 'training', 'progress': 0, 'message': 'å¼€å§‹è®­ç»ƒ...'}
        
        # åœ¨åå°çº¿ç¨‹ä¸­è®­ç»ƒ
        thread = threading.Thread(target=_train_text_classification_async)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'status': 'success',
            'message': 'è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·åœ¨ /nlp/train/status æŸ¥çœ‹è®­ç»ƒçŠ¶æ€'
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/nlp/train/sentiment', methods=['POST'])
def api_train_sentiment():
    """è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹æ¥å£"""
    if _training_status['sentiment']['status'] == 'training':
        return jsonify({
            'status': 'error',
            'message': 'æ¨¡å‹æ­£åœ¨è®­ç»ƒä¸­ï¼Œè¯·ç¨å€™...'
        }), 400
    
    try:
        # é‡ç½®çŠ¶æ€
        _training_status['sentiment'] = {'status': 'training', 'progress': 0, 'message': 'å¼€å§‹è®­ç»ƒ...'}
        
        # åœ¨åå°çº¿ç¨‹ä¸­è®­ç»ƒ
        thread = threading.Thread(target=_train_sentiment_async)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'status': 'success',
            'message': 'è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·åœ¨ /nlp/train/status æŸ¥çœ‹è®­ç»ƒçŠ¶æ€'
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/nlp/train/translation', methods=['POST'])
def api_train_translation():
    """è®­ç»ƒæœºå™¨ç¿»è¯‘æ¨¡å‹æ¥å£"""
    if _training_status['translation']['status'] == 'training':
        return jsonify({
            'status': 'error',
            'message': 'æ¨¡å‹æ­£åœ¨è®­ç»ƒä¸­ï¼Œè¯·ç¨å€™...'
        }), 400
    
    try:
        # é‡ç½®çŠ¶æ€
        _training_status['translation'] = {'status': 'training', 'progress': 0, 'message': 'å¼€å§‹è®­ç»ƒ...'}
        
        # åœ¨åå°çº¿ç¨‹ä¸­è®­ç»ƒ
        thread = threading.Thread(target=_train_translation_async)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'status': 'success',
            'message': 'è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·åœ¨ /nlp/train/status æŸ¥çœ‹è®­ç»ƒçŠ¶æ€'
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/nlp/train/status', methods=['GET'])
def api_training_status():
    """è·å–è®­ç»ƒçŠ¶æ€æ¥å£"""
    model_type = request.args.get('model', 'all')
    
    if model_type == 'all':
        return jsonify({
            'status': 'success',
            'training_status': _training_status
        })
    elif model_type in _training_status:
        return jsonify({
            'status': 'success',
            'model': model_type,
            'training_status': _training_status[model_type]
        })
    else:
        return jsonify({
            'status': 'error',
            'message': f'æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}'
        }), 400


@app.errorhandler(404)
def not_found(error):
    """
    å¤„ç†404é”™è¯¯
    """
    return jsonify({
        'status': 'error',
        'message': 'è¯·æ±‚çš„é¡µé¢ä¸å­˜åœ¨',
        'error': str(error)
    }), 404


@app.errorhandler(500)
def internal_error(error):
    """
    å¤„ç†500é”™è¯¯
    """
    return jsonify({
        'status': 'error',
        'message': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯',
        'error': str(error)
    }), 500


if __name__ == '__main__':
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨è±†åŒ…èŠå¤©æœºå™¨äººæœåŠ¡")
    print("=" * 60)
    
    # é¢„åŠ è½½NLPæ¨¡å‹
    preload_nlp_models()
    
    # æ£€æŸ¥æ¨¡æ¿æ–‡ä»¶
    index_path = os.path.join('templates', 'index.html')
    if os.path.exists(index_path):
        print(f"âœ… æ‰¾åˆ°æ¨¡æ¿æ–‡ä»¶: {index_path}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡æ¿æ–‡ä»¶: {index_path}")
        print("è¯·ç¡®ä¿ templates/index.html æ–‡ä»¶å­˜åœ¨")
    
    print("\nğŸ“¡ è®¿é—®åœ°å€:")
    print("   èŠå¤©ç•Œé¢: http://127.0.0.1:8808")
    print("   APIæµ‹è¯•: http://127.0.0.1:8808/test")
    print("   å¥åº·æ£€æŸ¥: http://127.0.0.1:8808/health")
    print("\nğŸ“ APIæ¥å£:")
    print("   POST /message - å‘é€å’Œæ¥æ”¶æ¶ˆæ¯")
    print("   POST /nlp/classify - æ–‡æœ¬åˆ†ç±»")
    print("   POST /nlp/sentiment - æƒ…æ„Ÿåˆ†æ")
    print("   POST /nlp/translate - æœºå™¨ç¿»è¯‘")
    print("\nğŸ”§ è®­ç»ƒæ¥å£:")
    print("   POST /nlp/train/text_classification - è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹")
    print("   POST /nlp/train/sentiment - è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹")
    print("   POST /nlp/train/translation - è®­ç»ƒæœºå™¨ç¿»è¯‘æ¨¡å‹")
    print("   GET  /nlp/train/status?model=<model_type> - æŸ¥çœ‹è®­ç»ƒçŠ¶æ€")
    print("=" * 60)
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(
        host='127.0.0.1',
        port=8808,
        debug=True,
        threaded=True  # å¯ç”¨å¤šçº¿ç¨‹
    )